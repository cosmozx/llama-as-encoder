{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebd7558-dbca-471e-9a78-71558bb3ba71",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed8fc3f-8d3f-4813-b407-b22b544e747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16767fa7-7044-4b3c-b438-444400adcc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import os\n",
    "from logging import getLogger\n",
    "from typing import List\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        # reload tokenizer\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n",
    "\n",
    "        # BOS / EOS token IDs\n",
    "        self.n_words: int = self.sp_model.vocab_size()\n",
    "        self.bos_id: int = self.sp_model.bos_id()\n",
    "        self.eos_id: int = self.sp_model.eos_id()\n",
    "        self.pad_id: int = self.sp_model.pad_id()\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        assert type(s) is str\n",
    "        t = self.sp_model.encode(s)\n",
    "        if bos:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self.sp_model.decode(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f226ee7-adb9-4d20-b979-47158a6e3de4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac548af-4d14-4703-9387-71f40f030cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_cis, mask\n",
    "        )\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "        print('mask', mask)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h_hat = self.norm(h)\n",
    "        print('h', h.shape)\n",
    "        output = self.output(h_hat).float()\n",
    "        print('output', output.shape)\n",
    "        return output, h.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42341dad-27fe-47a3-9fc4-1a4ebab44bdb",
   "metadata": {},
   "source": [
    "#  Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68db7849-7ecf-44df-9c70-ac6998edca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "\n",
    "from llama.model import ModelArgs, Transformer\n",
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "Dialog = List[Message]\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n",
    "UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\"\n",
    "\n",
    "\n",
    "class Llama:\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "    ) -> \"Llama\":\n",
    "        if not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(\"nccl\")\n",
    "        if not model_parallel_is_initialized():\n",
    "            if model_parallel_size is None:\n",
    "                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "            initialize_model_parallel(model_parallel_size)\n",
    "\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "        # seed must be the same in all processes\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        if local_rank > 0:\n",
    "            sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n",
    "        assert model_parallel_size == len(\n",
    "            checkpoints\n",
    "        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n",
    "        ckpt_path = checkpoints[get_model_parallel_rank()]\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params,\n",
    "        )\n",
    "        tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.n_words\n",
    "        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        model = Transformer(model_args)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        print(tokens)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            print(k, t)\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        hs = []\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits, h = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            print('l', logits.shape, torch.argmax(logits, dim=-1))\n",
    "            hs.append(h)\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                next_token == self.tokenizer.eos_id\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            if self.tokenizer.eos_id in toks:\n",
    "                eos_idx = toks.index(self.tokenizer.eos_id)\n",
    "                toks = toks[:eos_idx]\n",
    "                probs = probs[:eos_idx] if logprobs else None\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None, hs)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List[CompletionPrediction]:\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "        # print(prompt_tokens)\n",
    "        generation_tokens, generation_logprobs, hs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        print('g', generation_tokens)\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens], hs\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6b324-e95d-444b-897c-ab2e6f10a6cf",
   "metadata": {},
   "source": [
    "# Setup Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9d0ef9-2271-4e63-8069-c39c73869794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\n",
    "    os.environ[\"MASTER_ADDR\"] = 'localhost'\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    \"Cleans up the distributed environment\"\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d089ef-cfdf-48f4-8811-bd2900ab45b6",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc28539e-b840-4880-a3e9-d88d8ae722f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 13.66 seconds\n"
     ]
    }
   ],
   "source": [
    "setup(0, 1)\n",
    "\n",
    "\n",
    "ckpt_dir='/home/xingzha/Documents/llama-as-encoder/parameters/llama-2-7b'\n",
    "tokenizer_path='/home/xingzha/Documents/llama-as-encoder/parameters/tokenizer.model'\n",
    "temperature=0.6\n",
    "top_p=0.9\n",
    "max_seq_len=128\n",
    "max_gen_len=20\n",
    "max_batch_size=4\n",
    "\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(model_path=tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404796ab-4e72-4452-ab0d-f2517eab902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 1 2 -1\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.n_words,\n",
    "tokenizer.bos_id,\n",
    "tokenizer.eos_id,\n",
    "tokenizer.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd22fc8d-7057-4122-8fcd-397cd5de990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2694, 5465, 1476, 278, 321, 29922, 14047, 29985, 29906, 29889, 3439, 17632, 1925, 29892, 278, 6368, 310, 14215, 537, 5922, 393]]\n",
      "cur_pos 22\n",
      "h torch.Size([1, 22, 4096])\n",
      "output torch.Size([1, 22, 32000])\n",
      "l torch.Size([1, 22, 32000]) tensor([[  917,  5465, 29915,   393,  6368, 29922, 14047, 29906, 29906,  6306,\n",
      "          2694, 17632,  1925, 29892,   372,  6306,   310, 14215,   537,  5922,\n",
      "           393,   278]])\n",
      "cur_pos 23\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[6210]])\n",
      "cur_pos 24\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[310]])\n",
      "cur_pos 25\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[3578]])\n",
      "cur_pos 26\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[338]])\n",
      "cur_pos 27\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[263]])\n",
      "cur_pos 28\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[1021]])\n",
      "cur_pos 29\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[363]])\n",
      "cur_pos 30\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[599]])\n",
      "cur_pos 31\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[5366]])\n",
      "cur_pos 32\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[874]])\n",
      "cur_pos 33\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[29892]])\n",
      "cur_pos 34\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[393]])\n",
      "cur_pos 35\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[931]])\n",
      "cur_pos 36\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[322]])\n",
      "cur_pos 37\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[5864]])\n",
      "cur_pos 38\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[526]])\n",
      "cur_pos 39\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[1006]])\n",
      "cur_pos 40\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[3167]])\n",
      "cur_pos 41\n",
      "h torch.Size([1, 1, 4096])\n",
      "output torch.Size([1, 1, 32000])\n",
      "l torch.Size([1, 1, 32000]) tensor([[519]])\n",
      "g [[278, 6210, 310, 3578, 338, 278, 1021, 363, 599, 5366, 874, 322, 393, 4158, 322, 5864, 526, 1006, 3167, 519]]\n",
      "Einstein found the e=mc^2. Simply put, the theory of relativity states that\n",
      "> the speed of light is the same for all observers and that mass and energy are interchangeable\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = ['Einstein found the e=mc^2. Simply put, the theory of relativity states that']\n",
    "# prompts = ['equation of relativity states that the']\n",
    "results, hs = generator.text_completion(\n",
    "    prompts,\n",
    "    max_gen_len=max_gen_len,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    ")\n",
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3e10fbe-6813-4fd9-b120-dbef9db63a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tagsstein' that theory=mc22 equation Einply put, it equation of relativity states that the\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  917,  5465, 29915,   393,  6368, 29922, 14047, 29906, 29906,  6306,\n",
    "          2694, 17632,  1925, 29892,   372,  6306,   310, 14215,   537,  5922,\n",
    "           393,   278])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225eaab3-1089-434e-90ba-da2985ec0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "#     \"I believe the meaning of life is\",\n",
    "#     \"Simply put, the theory of relativity states that \",\n",
    "#     \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "#     Hi everyone,\n",
    "    \n",
    "#     I just \"\"\",\n",
    "#     # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "#     \"\"\"Translate English to French:\n",
    "    \n",
    "#     sea otter => loutre de mer\n",
    "#     peppermint => menthe poivrée\n",
    "#     plush girafe => girafe peluche\n",
    "#     cheese =>\"\"\",\n",
    "# ]\n",
    "# prompts = [\"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "#     Hi everyone,\n",
    "    \n",
    "#     I just \"\"\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82183a99-f791-4b49-bd11-bfd80782f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34e46c-03d9-4164-b80c-e563f4ced8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "250*4096 + 250*4096 + 250*4096 + 250*4096"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
