## Question answering
* NaturalQuestions https://ai.google.com/research/NaturalQuestions
* NarrativeQA https://github.com/deepmind/narrativeqa
* QuAC https://quac.ai/
* HellaSwag https://rowanzellers.com/hellaswag/
* OpenBookQA https://allenai.org/data/open-book-qa
* TruthfulQA https://github.com/sylinrl/TruthfulQA
* MMLU https://github.com/hendrycks/test
* SQuAD2.0 https://rajpurkar.github.io/SQuAD-explorer/
* BoolQ https://github.com/google-research-datasets/boolean-questions
* PIQA https://yonatanbisk.com/piqa/
* SIQA https://leaderboard.allenai.org/socialiqa/submissions/public
* WinoGrande https://winogrande.allenai.org/
* ARC 
    * https://allenai.org/data/arc
    * https://leaderboard.allenai.org/arc_easy/submissions/get-started
* CommonsenseQA https://www.tau-nlp.sites.tau.ac.il/commonsenseqa
* TriviaQA https://nlp.cs.washington.edu/triviaqa/
* Big Bench Hard https://github.com/suzgunmirac/BIG-Bench-Hard
* AGI Eval https://github.com/microsoft/AGIEval

## Information retrieval
* MS MARCO leaderboard (Nguyen et al. 2016; henceforth, the regular track) https://microsoft.github.io/msmarco/
* TREC 2019 Deep Learning track (Craswell et al. 2020; henceforth, the TREC track) https://microsoft.github.io/msmarco/TREC-Deep-Learning-2019.html

## Summarization
* CNN/DailyMail https://github.com/abisee/cnn-dailymail
* XSUM https://paperswithcode.com/dataset/xsum

## Sentiment analysis
* IMDB https://ai.stanford.edu/~amaas/data/sentiment/

## Reference
* https://arxiv.org/pdf/2307.09288.pdf
* https://arxiv.org/pdf/2211.09110.pdf
